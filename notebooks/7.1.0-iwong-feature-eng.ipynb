{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import bisect\n",
    "import numpy as np\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "input_filepath = '../data/interim/'\n",
    "output_filepath = '../data/processed/'\n",
    "\n",
    "# cols\n",
    "BINARY_COLS = Path.cwd().joinpath(input_filepath).joinpath('binary-cols.pickle')\n",
    "CATEGORICAL_COLS = Path.cwd().joinpath(input_filepath).joinpath('categorical-cols.pickle')\n",
    "CONTINUOUS_COLS = Path.cwd().joinpath(input_filepath).joinpath('continuous-cols.pickle')\n",
    "TARGET_COL = Path.cwd().joinpath(input_filepath).joinpath('target-col.pickle')\n",
    "\n",
    "BINARY_COLS_OUT = Path.cwd().joinpath(output_filepath).joinpath('binary-cols.pickle')\n",
    "CATEGORICAL_COLS_OUT = Path.cwd().joinpath(output_filepath).joinpath('categorical-cols.pickle')\n",
    "CONTINUOUS_COLS_OUT = Path.cwd().joinpath(output_filepath).joinpath('continuous-cols.pickle')\n",
    "TARGET_COL_OUT = Path.cwd().joinpath(output_filepath).joinpath('target-col.pickle')\n",
    "\n",
    "# data\n",
    "TRAIN_CSV = Path.cwd().joinpath(input_filepath).joinpath('train.csv')\n",
    "VAL_CSV = Path.cwd().joinpath(input_filepath).joinpath('val.csv')\n",
    "TEST_CSV = Path.cwd().joinpath(input_filepath).joinpath('test.csv')\n",
    "\n",
    "TRAIN_CSV_OUT = Path.cwd().joinpath(output_filepath).joinpath('train.csv')\n",
    "VAL_CSV_OUT = Path.cwd().joinpath(output_filepath).joinpath('val.csv')\n",
    "TEST_CSV_OUT = Path.cwd().joinpath(output_filepath).joinpath('test.csv')\n",
    "\n",
    "# metadata\n",
    "BINARY_ENCODERS = Path.cwd().joinpath(output_filepath).joinpath('binary-encoders.pickle')\n",
    "CATEGORICAL_ENCODERS = Path.cwd().joinpath(output_filepath).joinpath('categorical-encoders.pickle')\n",
    "TARGET_ENCODERS = Path.cwd().joinpath(output_filepath).joinpath('target-encoders.pickle')\n",
    "CONTINUOUS_SCALERS = Path.cwd().joinpath(output_filepath).joinpath('continuous-scalers.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def read_obj(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "binary_cols = read_obj(BINARY_COLS)\n",
    "categorical_cols = read_obj(CATEGORICAL_COLS)\n",
    "continuous_cols = read_obj(CONTINUOUS_COLS)\n",
    "target_col = read_obj(TARGET_COL)\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "val = pd.read_csv(VAL_CSV)\n",
    "test = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Label Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pair_cols = ['ethnicity',\n",
    " 'gender',\n",
    " 'hospital_admit_source',\n",
    " 'icu_admit_source',\n",
    " 'icu_stay_type',\n",
    " 'icu_type',\n",
    " 'apache_3j_bodysystem',\n",
    " 'apache_2_bodysystem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cmbs = list(combinations(pair_cols, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def concat_columns(df, columns):\n",
    "    value = df[columns[0]].astype(str) + ' '\n",
    "    for col in columns[1:]:\n",
    "        value += df[col].astype(str) + ' '\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cmbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "combo_cols = list()\n",
    "for cols in cmbs:\n",
    "    col_name = f'paired_{\"_\".join(cols)}'\n",
    "    combo_cols.append(col_name)\n",
    "    train[col_name] = concat_columns(train, cols)\n",
    "    val[col_name] = concat_columns(val, cols)\n",
    "    test[col_name] = concat_columns(test, cols)\n",
    "categorical_cols.extend(combo_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# aggregate icu\n",
    "train['hospital_admit_source_is_icu'] = train['hospital_admit_source'].apply(\n",
    "        lambda x: \n",
    "            'True' if x in [\n",
    "                'Other ICU', \n",
    "                'ICU to SDU',\n",
    "                'ICU'\n",
    "            ] else 'False')\n",
    "val['hospital_admit_source_is_icu'] = val['hospital_admit_source'].apply(\n",
    "        lambda x: \n",
    "            'True' if x in [\n",
    "                'Other ICU', \n",
    "                'ICU to SDU',\n",
    "                'ICU'\n",
    "            ] else 'False')\n",
    "test['hospital_admit_source_is_icu'] = test['hospital_admit_source'].apply(\n",
    "        lambda x: \n",
    "            'True' if x in [\n",
    "                'Other ICU', \n",
    "                'ICU to SDU',\n",
    "                'ICU'\n",
    "            ] else 'False')\n",
    "categorical_cols.append('hospital_admit_source_is_icu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# aggregate cardiac\n",
    "common_cols = ['CTICU', 'CCU-CTICU', 'Cardiac ICU', 'CSICU']\n",
    "train['hospital_admit_source_is_cardiac'] = train['icu_type'].apply(lambda x: True if x in common_cols else False)\n",
    "val['hospital_admit_source_is_cardiac'] = val['icu_type'].apply(lambda x: True if x in common_cols else False)\n",
    "test['hospital_admit_source_is_cardiac'] = test['icu_type'].apply(lambda x: True if x in common_cols else False)\n",
    "categorical_cols.append('hospital_admit_source_is_cardiac')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Typify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train[continuous_cols] = train[continuous_cols].astype('float32')\n",
    "val[continuous_cols] = val[continuous_cols].astype('float32')\n",
    "test[continuous_cols] = test[continuous_cols].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train[categorical_cols] = train[categorical_cols].astype('str').astype('category')\n",
    "val[categorical_cols] = val[categorical_cols].astype('str').astype('category')\n",
    "test[categorical_cols] = test[categorical_cols].astype('str').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train[binary_cols] = train[binary_cols].astype('str').astype('category')\n",
    "val[binary_cols] = val[binary_cols].astype('str').astype('category')\n",
    "test[binary_cols] = test[binary_cols].astype('str').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train[target_col] = train[target_col].astype('str').astype('category')\n",
    "val[target_col] = val[target_col].astype('str').astype('category')\n",
    "test[target_col] = test[target_col].astype('str').astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train = train.dropna(how='all')\n",
    "val = val.dropna(how='all')\n",
    "test = test.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "confounding_cols = ['apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob']\n",
    "\n",
    "# remove confounding vars - biases and undue variance\n",
    "for x in confounding_cols:\n",
    "    continuous_cols.remove(x)\n",
    "    \n",
    "train = train[[target_col] + continuous_cols + categorical_cols + binary_cols]\n",
    "val = val[[target_col] + continuous_cols + categorical_cols + binary_cols]\n",
    "test = test[[target_col] + continuous_cols + categorical_cols + binary_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Fill Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def fill(df, cols, fillers=None):\n",
    "    if None is fillers:\n",
    "        fillers = dict()\n",
    "        \n",
    "        for col in cols:\n",
    "            for hospital_id in df['hospital_id'].unique():\n",
    "\n",
    "                subset = df[df['hospital_id'] == hospital_id]\n",
    "                hospital_col_key = f'{col}_{hospital_id}'\n",
    "\n",
    "                if hospital_col_key not in fillers:\n",
    "                    fillers[hospital_col_key] = subset[col].dropna().median()\n",
    "            \n",
    "            fillers[col] = df[col].dropna().median()\n",
    "    \n",
    "    for col in cols:\n",
    "        print(f'fillling {col}')\n",
    "        for hospital_id in df['hospital_id'].unique():\n",
    "            print(f'fillling {col} - {hospital_id}')\n",
    "            subset = df[df['hospital_id'] == hospital_id]\n",
    "            hospital_col_key = f'{col}_{hospital_id}'\n",
    "            \n",
    "            if hospital_col_key in fillers:\n",
    "                df.loc[df['hospital_id'] == hospital_id, col] = subset[col].fillna(fillers[hospital_col_key])\n",
    "            else:\n",
    "                df.loc[df['hospital_id'] == hospital_id, col] = subset[col].fillna(fillers[col])\n",
    "            \n",
    "            df.loc[df['hospital_id'] == hospital_id, f'{col}_na'] = pd.isnull(subset[col])\n",
    "    \n",
    "    return df, fillers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fillling encounter_id\n",
      "fillling encounter_id - 118.0\n",
      "fillling encounter_id - 81.0\n",
      "fillling encounter_id - 33.0\n",
      "fillling encounter_id - 83.0\n",
      "fillling encounter_id - 77.0\n",
      "fillling encounter_id - 149.0\n",
      "fillling encounter_id - 31.0\n",
      "fillling encounter_id - 151.0\n",
      "fillling encounter_id - 69.0\n",
      "fillling encounter_id - 46.0\n",
      "fillling encounter_id - 63.0\n",
      "fillling encounter_id - 113.0\n",
      "fillling encounter_id - 137.0\n",
      "fillling encounter_id - 89.0\n",
      "fillling encounter_id - 168.0\n",
      "fillling encounter_id - 150.0\n",
      "fillling encounter_id - 140.0\n",
      "fillling encounter_id - 34.0\n",
      "fillling encounter_id - 74.0\n",
      "fillling encounter_id - 102.0\n",
      "fillling encounter_id - 91.0\n",
      "fillling encounter_id - 198.0\n",
      "fillling encounter_id - 129.0\n",
      "fillling encounter_id - 68.0\n",
      "fillling encounter_id - 138.0\n",
      "fillling encounter_id - 174.0\n",
      "fillling encounter_id - 134.0\n",
      "fillling encounter_id - 169.0\n",
      "fillling encounter_id - 57.0\n",
      "fillling encounter_id - 177.0\n",
      "fillling encounter_id - 155.0\n",
      "fillling encounter_id - 85.0\n",
      "fillling encounter_id - 78.0\n",
      "fillling encounter_id - 98.0\n",
      "fillling encounter_id - 105.0\n",
      "fillling encounter_id - 76.0\n",
      "fillling encounter_id - 5.0\n",
      "fillling encounter_id - 116.0\n",
      "fillling encounter_id - 119.0\n",
      "fillling encounter_id - 79.0\n",
      "fillling encounter_id - 171.0\n",
      "fillling encounter_id - 161.0\n",
      "fillling encounter_id - 128.0\n",
      "fillling encounter_id - 147.0\n",
      "fillling encounter_id - 112.0\n",
      "fillling encounter_id - 32.0\n",
      "fillling encounter_id - 55.0\n",
      "fillling encounter_id - 17.0\n",
      "fillling encounter_id - 136.0\n",
      "fillling encounter_id - 44.0\n",
      "fillling encounter_id - 179.0\n",
      "fillling encounter_id - 88.0\n",
      "fillling encounter_id - 72.0\n",
      "fillling encounter_id - 109.0\n",
      "fillling encounter_id - 10.0\n",
      "fillling encounter_id - 204.0\n",
      "fillling encounter_id - 117.0\n",
      "fillling encounter_id - 202.0\n",
      "fillling encounter_id - 70.0\n",
      "fillling encounter_id - 160.0\n",
      "fillling encounter_id - 62.0\n",
      "fillling encounter_id - 16.0\n",
      "fillling encounter_id - 199.0\n",
      "fillling encounter_id - 54.0\n",
      "fillling encounter_id - 100.0\n",
      "fillling encounter_id - 186.0\n",
      "fillling encounter_id - 20.0\n",
      "fillling encounter_id - 107.0\n",
      "fillling encounter_id - 14.0\n",
      "fillling encounter_id - 132.0\n",
      "fillling encounter_id - 87.0\n",
      "fillling encounter_id - 53.0\n",
      "fillling encounter_id - 21.0\n",
      "fillling encounter_id - 99.0\n",
      "fillling encounter_id - 71.0\n",
      "fillling encounter_id - 182.0\n",
      "fillling encounter_id - 24.0\n",
      "fillling encounter_id - 200.0\n",
      "fillling encounter_id - 189.0\n",
      "fillling encounter_id - 60.0\n",
      "fillling encounter_id - 84.0\n",
      "fillling encounter_id - 26.0\n",
      "fillling encounter_id - 133.0\n",
      "fillling encounter_id - 27.0\n",
      "fillling encounter_id - 8.0\n",
      "fillling encounter_id - 194.0\n",
      "fillling encounter_id - 111.0\n",
      "fillling encounter_id - 176.0\n",
      "fillling encounter_id - 187.0\n",
      "fillling encounter_id - 39.0\n",
      "fillling encounter_id - 101.0\n",
      "fillling encounter_id - 80.0\n",
      "fillling encounter_id - 90.0\n",
      "fillling encounter_id - 19.0\n",
      "fillling encounter_id - 94.0\n",
      "fillling encounter_id - 50.0\n",
      "fillling encounter_id - 192.0\n",
      "fillling encounter_id - 43.0\n",
      "fillling encounter_id - 145.0\n",
      "fillling encounter_id - 180.0\n",
      "fillling encounter_id - 40.0\n",
      "fillling encounter_id - 9.0\n",
      "fillling encounter_id - 185.0\n",
      "fillling encounter_id - 159.0\n",
      "fillling encounter_id - 166.0\n",
      "fillling encounter_id - 157.0\n",
      "fillling encounter_id - 64.0\n",
      "fillling encounter_id - 125.0\n",
      "fillling encounter_id - 92.0\n",
      "fillling encounter_id - 13.0\n",
      "fillling encounter_id - 37.0\n",
      "fillling encounter_id - 139.0\n",
      "fillling encounter_id - 135.0\n",
      "fillling encounter_id - 51.0\n",
      "fillling encounter_id - 18.0\n",
      "fillling encounter_id - 181.0\n",
      "fillling encounter_id - 158.0\n",
      "fillling encounter_id - 196.0\n",
      "fillling encounter_id - 47.0\n",
      "fillling encounter_id - 103.0\n",
      "fillling encounter_id - 197.0\n",
      "fillling encounter_id - 188.0\n",
      "fillling encounter_id - 2.0\n",
      "fillling encounter_id - 49.0\n",
      "fillling encounter_id - 142.0\n",
      "fillling encounter_id - 35.0\n",
      "fillling encounter_id - 15.0\n",
      "fillling encounter_id - 146.0\n",
      "fillling encounter_id - 29.0\n",
      "fillling encounter_id - 36.0\n",
      "fillling encounter_id - 30.0\n",
      "fillling encounter_id - 184.0\n",
      "fillling encounter_id - 104.0\n",
      "fillling encounter_id - 121.0\n",
      "fillling encounter_id - 195.0\n",
      "fillling encounter_id - 3.0\n",
      "fillling encounter_id - 6.0\n",
      "fillling encounter_id - 183.0\n",
      "fillling encounter_id - 66.0\n",
      "fillling encounter_id\n",
      "fillling encounter_id - 118.0\n",
      "fillling encounter_id - 81.0\n",
      "fillling encounter_id - 33.0\n",
      "fillling encounter_id - 83.0\n",
      "fillling encounter_id - 77.0\n",
      "fillling encounter_id - 149.0\n",
      "fillling encounter_id - 31.0\n",
      "fillling encounter_id - 151.0\n",
      "fillling encounter_id - 69.0\n",
      "fillling encounter_id - 46.0\n",
      "fillling encounter_id - 63.0\n",
      "fillling encounter_id - 113.0\n",
      "fillling encounter_id - 137.0\n",
      "fillling encounter_id - 89.0\n",
      "fillling encounter_id - 168.0\n",
      "fillling encounter_id - 150.0\n",
      "fillling encounter_id - 140.0\n",
      "fillling encounter_id - 34.0\n",
      "fillling encounter_id - 74.0\n",
      "fillling encounter_id - 102.0\n",
      "fillling encounter_id - 91.0\n",
      "fillling encounter_id - 198.0\n",
      "fillling encounter_id - 129.0\n",
      "fillling encounter_id - 68.0\n",
      "fillling encounter_id - 138.0\n",
      "fillling encounter_id - 174.0\n",
      "fillling encounter_id - 134.0\n",
      "fillling encounter_id - 169.0\n",
      "fillling encounter_id - 57.0\n",
      "fillling encounter_id - 177.0\n",
      "fillling encounter_id - 155.0\n",
      "fillling encounter_id - 85.0\n",
      "fillling encounter_id - 78.0\n",
      "fillling encounter_id - 98.0\n",
      "fillling encounter_id - 105.0\n",
      "fillling encounter_id - 76.0\n",
      "fillling encounter_id - 5.0\n",
      "fillling encounter_id - 116.0\n",
      "fillling encounter_id - 119.0\n",
      "fillling encounter_id - 79.0\n",
      "fillling encounter_id - 171.0\n",
      "fillling encounter_id - 161.0\n",
      "fillling encounter_id - 128.0\n",
      "fillling encounter_id - 147.0\n",
      "fillling encounter_id - 112.0\n",
      "fillling encounter_id - 32.0\n",
      "fillling encounter_id - 55.0\n",
      "fillling encounter_id - 17.0\n",
      "fillling encounter_id - 136.0\n",
      "fillling encounter_id - 44.0\n",
      "fillling encounter_id - 179.0\n",
      "fillling encounter_id - 88.0\n",
      "fillling encounter_id - 72.0\n",
      "fillling encounter_id - 109.0\n",
      "fillling encounter_id - 10.0\n",
      "fillling encounter_id - 204.0\n",
      "fillling encounter_id - 117.0\n",
      "fillling encounter_id - 202.0\n",
      "fillling encounter_id - 70.0\n",
      "fillling encounter_id - 160.0\n",
      "fillling encounter_id - 62.0\n",
      "fillling encounter_id - 16.0\n",
      "fillling encounter_id - 199.0\n",
      "fillling encounter_id - 54.0\n",
      "fillling encounter_id - 100.0\n",
      "fillling encounter_id - 186.0\n",
      "fillling encounter_id - 20.0\n",
      "fillling encounter_id - 107.0\n",
      "fillling encounter_id - 14.0\n",
      "fillling encounter_id - 132.0\n",
      "fillling encounter_id - 87.0\n",
      "fillling encounter_id - 53.0\n",
      "fillling encounter_id - 21.0\n",
      "fillling encounter_id - 99.0\n",
      "fillling encounter_id - 71.0\n",
      "fillling encounter_id - 182.0\n",
      "fillling encounter_id - 24.0\n",
      "fillling encounter_id - 200.0\n",
      "fillling encounter_id - 189.0\n",
      "fillling encounter_id - 60.0\n",
      "fillling encounter_id - 84.0\n",
      "fillling encounter_id - 26.0\n",
      "fillling encounter_id - 133.0\n",
      "fillling encounter_id - 27.0\n",
      "fillling encounter_id - 8.0\n",
      "fillling encounter_id - 194.0\n",
      "fillling encounter_id - 111.0\n",
      "fillling encounter_id - 176.0\n",
      "fillling encounter_id - 187.0\n",
      "fillling encounter_id - 39.0\n",
      "fillling encounter_id - 101.0\n",
      "fillling encounter_id - 80.0\n",
      "fillling encounter_id - 90.0\n",
      "fillling encounter_id - 19.0\n",
      "fillling encounter_id - 94.0\n",
      "fillling encounter_id - 50.0\n",
      "fillling encounter_id - 192.0\n",
      "fillling encounter_id - 43.0\n",
      "fillling encounter_id - 145.0\n",
      "fillling encounter_id - 180.0\n",
      "fillling encounter_id - 40.0\n",
      "fillling encounter_id - 9.0\n",
      "fillling encounter_id - 185.0\n",
      "fillling encounter_id - 159.0\n",
      "fillling encounter_id - 166.0\n",
      "fillling encounter_id - 157.0\n",
      "fillling encounter_id - 64.0\n",
      "fillling encounter_id - 125.0\n",
      "fillling encounter_id - 92.0\n",
      "fillling encounter_id - 13.0\n",
      "fillling encounter_id - 37.0\n",
      "fillling encounter_id - 139.0\n",
      "fillling encounter_id - 135.0\n",
      "fillling encounter_id - 51.0\n",
      "fillling encounter_id - 18.0\n",
      "fillling encounter_id - 181.0\n",
      "fillling encounter_id - 158.0\n",
      "fillling encounter_id - 196.0\n",
      "fillling encounter_id - 47.0\n",
      "fillling encounter_id - 103.0\n",
      "fillling encounter_id - 197.0\n",
      "fillling encounter_id - 188.0\n",
      "fillling encounter_id - 2.0\n",
      "fillling encounter_id - 49.0\n",
      "fillling encounter_id - 142.0\n",
      "fillling encounter_id - 35.0\n",
      "fillling encounter_id - 15.0\n",
      "fillling encounter_id - 146.0\n",
      "fillling encounter_id - 29.0\n",
      "fillling encounter_id - 36.0\n",
      "fillling encounter_id - 30.0\n",
      "fillling encounter_id - 184.0\n",
      "fillling encounter_id - 104.0\n",
      "fillling encounter_id - 121.0\n",
      "fillling encounter_id - 195.0\n",
      "fillling encounter_id - 3.0\n",
      "fillling encounter_id - 6.0\n",
      "fillling encounter_id - 183.0\n",
      "fillling encounter_id - 66.0\n",
      "fillling encounter_id\n",
      "fillling encounter_id - 118.0\n",
      "fillling encounter_id - 81.0\n",
      "fillling encounter_id - 33.0\n",
      "fillling encounter_id - 83.0\n",
      "fillling encounter_id - 77.0\n",
      "fillling encounter_id - 149.0\n",
      "fillling encounter_id - 31.0\n",
      "fillling encounter_id - 151.0\n",
      "fillling encounter_id - 69.0\n",
      "fillling encounter_id - 46.0\n",
      "fillling encounter_id - 63.0\n",
      "fillling encounter_id - 113.0\n",
      "fillling encounter_id - 137.0\n",
      "fillling encounter_id - 89.0\n",
      "fillling encounter_id - 168.0\n",
      "fillling encounter_id - 150.0\n",
      "fillling encounter_id - 140.0\n",
      "fillling encounter_id - 34.0\n",
      "fillling encounter_id - 74.0\n",
      "fillling encounter_id - 102.0\n",
      "fillling encounter_id - 91.0\n",
      "fillling encounter_id - 198.0\n",
      "fillling encounter_id - 129.0\n",
      "fillling encounter_id - 68.0\n",
      "fillling encounter_id - 138.0\n",
      "fillling encounter_id - 174.0\n",
      "fillling encounter_id - 134.0\n",
      "fillling encounter_id - 169.0\n",
      "fillling encounter_id - 57.0\n",
      "fillling encounter_id - 177.0\n",
      "fillling encounter_id - 155.0\n",
      "fillling encounter_id - 85.0\n",
      "fillling encounter_id - 78.0\n",
      "fillling encounter_id - 98.0\n",
      "fillling encounter_id - 105.0\n",
      "fillling encounter_id - 76.0\n",
      "fillling encounter_id - 5.0\n",
      "fillling encounter_id - 116.0\n",
      "fillling encounter_id - 119.0\n",
      "fillling encounter_id - 79.0\n",
      "fillling encounter_id - 171.0\n",
      "fillling encounter_id - 161.0\n",
      "fillling encounter_id - 128.0\n",
      "fillling encounter_id - 147.0\n",
      "fillling encounter_id - 112.0\n",
      "fillling encounter_id - 32.0\n",
      "fillling encounter_id - 55.0\n",
      "fillling encounter_id - 17.0\n",
      "fillling encounter_id - 136.0\n",
      "fillling encounter_id - 44.0\n",
      "fillling encounter_id - 179.0\n",
      "fillling encounter_id - 88.0\n",
      "fillling encounter_id - 72.0\n",
      "fillling encounter_id - 109.0\n",
      "fillling encounter_id - 10.0\n",
      "fillling encounter_id - 204.0\n",
      "fillling encounter_id - 117.0\n",
      "fillling encounter_id - 202.0\n",
      "fillling encounter_id - 70.0\n",
      "fillling encounter_id - 160.0\n",
      "fillling encounter_id - 62.0\n",
      "fillling encounter_id - 16.0\n",
      "fillling encounter_id - 199.0\n",
      "fillling encounter_id - 54.0\n",
      "fillling encounter_id - 100.0\n",
      "fillling encounter_id - 186.0\n",
      "fillling encounter_id - 20.0\n",
      "fillling encounter_id - 107.0\n",
      "fillling encounter_id - 14.0\n",
      "fillling encounter_id - 132.0\n",
      "fillling encounter_id - 87.0\n",
      "fillling encounter_id - 53.0\n",
      "fillling encounter_id - 21.0\n",
      "fillling encounter_id - 99.0\n",
      "fillling encounter_id - 71.0\n",
      "fillling encounter_id - 182.0\n",
      "fillling encounter_id - 24.0\n",
      "fillling encounter_id - 200.0\n",
      "fillling encounter_id - 189.0\n",
      "fillling encounter_id - 60.0\n",
      "fillling encounter_id - 84.0\n",
      "fillling encounter_id - 26.0\n",
      "fillling encounter_id - 133.0\n",
      "fillling encounter_id - 27.0\n",
      "fillling encounter_id - 8.0\n",
      "fillling encounter_id - 194.0\n",
      "fillling encounter_id - 111.0\n",
      "fillling encounter_id - 176.0\n",
      "fillling encounter_id - 187.0\n",
      "fillling encounter_id - 39.0\n",
      "fillling encounter_id - 101.0\n",
      "fillling encounter_id - 80.0\n",
      "fillling encounter_id - 90.0\n",
      "fillling encounter_id - 19.0\n",
      "fillling encounter_id - 94.0\n",
      "fillling encounter_id - 50.0\n",
      "fillling encounter_id - 192.0\n",
      "fillling encounter_id - 43.0\n",
      "fillling encounter_id - 145.0\n",
      "fillling encounter_id - 180.0\n",
      "fillling encounter_id - 40.0\n",
      "fillling encounter_id - 9.0\n",
      "fillling encounter_id - 185.0\n",
      "fillling encounter_id - 159.0\n",
      "fillling encounter_id - 166.0\n",
      "fillling encounter_id - 157.0\n",
      "fillling encounter_id - 64.0\n",
      "fillling encounter_id - 125.0\n",
      "fillling encounter_id - 92.0\n",
      "fillling encounter_id - 13.0\n",
      "fillling encounter_id - 37.0\n",
      "fillling encounter_id - 139.0\n",
      "fillling encounter_id - 135.0\n",
      "fillling encounter_id - 51.0\n",
      "fillling encounter_id - 18.0\n",
      "fillling encounter_id - 181.0\n",
      "fillling encounter_id - 158.0\n",
      "fillling encounter_id - 196.0\n",
      "fillling encounter_id - 47.0\n",
      "fillling encounter_id - 103.0\n",
      "fillling encounter_id - 197.0\n",
      "fillling encounter_id - 188.0\n",
      "fillling encounter_id - 2.0\n",
      "fillling encounter_id - 49.0\n",
      "fillling encounter_id - 142.0\n",
      "fillling encounter_id - 35.0\n",
      "fillling encounter_id - 15.0\n",
      "fillling encounter_id - 146.0\n",
      "fillling encounter_id - 29.0\n",
      "fillling encounter_id - 36.0\n",
      "fillling encounter_id - 30.0\n",
      "fillling encounter_id - 184.0\n",
      "fillling encounter_id - 104.0\n",
      "fillling encounter_id - 121.0\n",
      "fillling encounter_id - 195.0\n",
      "fillling encounter_id - 3.0\n",
      "fillling encounter_id - 6.0\n",
      "fillling encounter_id - 183.0\n",
      "fillling encounter_id - 66.0\n"
     ]
    }
   ],
   "source": [
    "train, fillers = fill(train, continuous_cols)\n",
    "val, _ = fill(val, continuous_cols, fillers)\n",
    "test, _ = fill(test, continuous_cols, fillers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "categorical_cols.extend([f'{x}_na' for x in continuous_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def normalize(df, cols, scalers=None):\n",
    "    if None is scalers:\n",
    "        scalers = dict()\n",
    "        \n",
    "    for col in cols:\n",
    "        if col not in scalers:\n",
    "            scalers[col] = StandardScaler(with_mean=True, with_std=True)\n",
    "            scalers[col].fit(df[col].values.reshape(-1,1))\n",
    "        \n",
    "        scaler = scalers[col]\n",
    "        df[col] = scaler.transform(df[col].values.reshape(-1,1))\n",
    "    return df, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train, scalers = normalize(train, continuous_cols)\n",
    "val, _ = normalize(val, continuous_cols, scalers)\n",
    "test, _ = normalize(test, continuous_cols, scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train[continuous_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Label Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train[categorical_cols] = train[categorical_cols].astype('str').astype('category')\n",
    "val[categorical_cols] = val[categorical_cols].astype('str').astype('category')\n",
    "test[categorical_cols] = test[categorical_cols].astype('str').astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def labelencode(df, cols, encoders=None, unknown_value='UNK'):\n",
    "    if None is encoders:\n",
    "        encoders = dict()\n",
    "        \n",
    "    for col in cols:\n",
    "        if col not in encoders:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(df[col].values)\n",
    "            \n",
    "            # add unknown val to cats\n",
    "            cats = le.classes_.tolist()\n",
    "            bisect.insort_left(cats, unknown_value)\n",
    "            \n",
    "            # redefine cats on le\n",
    "            le.classes_ = np.asarray(cats)\n",
    "\n",
    "            encoders[col] = le\n",
    "        \n",
    "        le = encoders[col]\n",
    "        df[col] = df[col].map(lambda x: unknown_value if x not in le.classes_ else x)\n",
    "        df[col] = le.transform(df[col].values)\n",
    "        \n",
    "    return df, encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train, label_encoders = labelencode(train, categorical_cols)\n",
    "val, _ = labelencode(val, categorical_cols, label_encoders)\n",
    "test, _ = labelencode(test, categorical_cols, label_encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train[categorical_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## One-Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# todo - not necessary with CatBoost, plut CBoost will tune the cats which will become ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train, ohe_encoders = labelencode(train, binary_cols)\n",
    "val, _ = labelencode(val, binary_cols, ohe_encoders)\n",
    "test, _ = labelencode(test, binary_cols, ohe_encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train[binary_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Label Encode Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train, target_encoders = labelencode(train, [target_col])\n",
    "val, _ = labelencode(val, [target_col], target_encoders)\n",
    "test, _ = labelencode(test, [target_col], target_encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train[target_col].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Persist Data and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def pickle_obj(path, obj):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# cols\n",
    "pickle_obj(BINARY_COLS_OUT, binary_cols)\n",
    "pickle_obj(CATEGORICAL_COLS_OUT, categorical_cols)\n",
    "pickle_obj(CONTINUOUS_COLS_OUT, continuous_cols)\n",
    "pickle_obj(TARGET_COL_OUT, target_col)\n",
    "\n",
    "# metadata\n",
    "pickle_obj(BINARY_ENCODERS, ohe_encoders)\n",
    "pickle_obj(CATEGORICAL_ENCODERS, label_encoders)\n",
    "pickle_obj(TARGET_ENCODERS, target_encoders)\n",
    "pickle_obj(CONTINUOUS_SCALERS, scalers)\n",
    "\n",
    "# data\n",
    "train.to_csv(TRAIN_CSV_OUT, index=False)\n",
    "val.to_csv(VAL_CSV_OUT, index=False)\n",
    "test.to_csv(TEST_CSV_OUT, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wids-datathon-2020",
   "language": "python",
   "name": "valence"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
