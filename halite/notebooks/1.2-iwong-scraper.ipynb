{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "__Purpose of this notebook is to design and implement a scraper of kaggle halite episode/game replays.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Design Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "- there are two endpoints:\n",
    "    - one that provides episode/game replays\n",
    "    - the other which provides episode, team and agent metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "- we need datasets to make data-driven decisions when building our halite agent\n",
    "    - we can fulfill that data-dependency by obtaining historical gameplay\n",
    "        - we can use the GetEpisodeReplay Endpoint to obtain game replays\n",
    "- we want to pull a diverse set of game replays from good and bad bots\n",
    "    - we can rank and sort the gameplays by their scores\n",
    "        - we can obtain scores and metadata for episodes from the ListEpisodes Endpoint\n",
    "- the ListEpisodes Endpoint can be queried by TeamId, SubmissionId, EpisodeIds\n",
    "    - likely there are the fewest team ids but they are tied to all episodes\n",
    "        - we can crawl through the an arbitrary team's oponents and build out a set of teams\n",
    "            - for each team we can download their episodes based on some episode priority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Design Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "- persist enumerated teamIds\n",
    "- persist downloaded episode ids\n",
    "- persist enumerated episode ids\n",
    "- rate limited\n",
    "    - 60 requests per minute max\n",
    "    - 1000 requests per day max (167 mins @ 60 reqs/sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Implement Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from src.api.KaggleClient import KaggleClient\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def synchronize_disk(filepath, data, overwrite=False):\n",
    "    synchronized_data = data\n",
    "    if isinstance(synchronized_data, list) or isinstance(synchronized_data, set):\n",
    "        if not overwrite:\n",
    "            try:\n",
    "                with open(filepath, 'r') as f:\n",
    "                    synchronized_data.update([int(x) for x in f.readlines()])\n",
    "            except FileNotFoundError:\n",
    "                logging.warn('Failed to synchronize from disk. File {} is empty.'.format(filepath))\n",
    "        try:\n",
    "            with open(filepath, 'w+') as f:\n",
    "                for x in synchronized_data:\n",
    "                    f.write('{}\\n'.format(x))\n",
    "        except Exception:\n",
    "            logging.exception('Failed to synchronize to disk.')\n",
    "    elif isinstance(synchronized_data, dict):\n",
    "        if not overwrite:\n",
    "            try:\n",
    "                with open(filepath, 'r') as f:\n",
    "                    synchronized_data.update(json.load(f))\n",
    "            except FileNotFoundError:\n",
    "                logging.warn('Failed to synchronize from disk. File {} is empty.'.format(filepath))\n",
    "        try:\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(synchronized_data, f)\n",
    "        except Exception:\n",
    "            logging.exception('Failed to synchronize to disk.')\n",
    "\n",
    "    return synchronized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_team_ids(episode_metadata):\n",
    "    team_ids = {episode_metadata['result']['teams'][i]['id'] for i in range(len(episode_metadata['result']['teams']))}\n",
    "    team_ids.update({agent['submission']['teamId'] for episode in episode_metadata['result']['episodes'] for agent in episode['agents']})\n",
    "    return team_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_episode_ids(episode_metadata):\n",
    "    episode_ids = {episode['id'] for episode in episode_metadata['result']['episodes']}\n",
    "    return episode_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def save_json(filepath, json_data):\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(json_data, f)\n",
    "        #f.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_episode_priority_map(episode_metadata):\n",
    "    priority_map = dict()\n",
    "    for episode in episode_metadata['result']['episodes']:\n",
    "        episode_id = str(episode['id'])\n",
    "        weights = [agent['updatedScore'] for agent in episode['agents'] if None is not agent['updatedScore']]\n",
    "        priority = sum(weights)/len(weights) if 0 < len(weights) else 0\n",
    "        priority_map[episode_id] = priority\n",
    "    return priority_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_team_priority_map(episode_metadata):\n",
    "    priority_map = dict()\n",
    "    for team in episode_metadata['result']['teams']:\n",
    "        team_id = str(team['id'])\n",
    "        priority = team['publicLeaderboardRank']\n",
    "        priority_map[team_id] = priority\n",
    "    return priority_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def synchronize_priority_map(priority_map, filepath, stale_priorities=None):\n",
    "    stale_priorities = stale_priorities if None is not stale_priorities else set()\n",
    "    priority_map = synchronize_disk(filepath, priority_map)\n",
    "    priority_map = {k:v for k,v in priority_map.items() if k not in stale_priorities}\n",
    "    priority_map = synchronize_disk(filepath, priority_map, overwrite=True)\n",
    "    return priority_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_min_weighted_priority_queue(priority_map, watermark, request_budget=None):\n",
    "    return get_priority_queue(priority_map, watermark, max_weighted=False, request_budget=request_budget)\n",
    "\n",
    "def get_max_weighted_priority_queue(priority_map, watermark, request_budget=None):\n",
    "    return get_priority_queue(priority_map, watermark, max_weighted=True, request_budget=request_budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def get_priority_queue(priorty_map, watermark, max_weighted=True, request_budget=None):\n",
    "    prioritized_keys = sorted(priorty_map, key=priorty_map.get, reverse=max_weighted)\n",
    "    watermark_fn = lambda x: (max_weighted and watermark <= x) or (not max_weighted and watermark >= x)\n",
    "    watermarked_keys = [k for k in prioritized_keys if watermark_fn(priorty_map[k])]\n",
    "    ratelimited_keys = watermarked_keys[:request_budget]\n",
    "    return [(k, priorty_map[k]) for k in ratelimited_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "EPISODE_WATERMARK = 1100\n",
    "TEAM_WATERMARK = 25\n",
    "REQUEST_LIMIT =  10 #60*3 # must be smaller than 1000\n",
    "REQUEST_DISCOVERY_BUDGET = 0.5\n",
    "ARBITRARY_TEAM_ID = '5118174'\n",
    "\n",
    "DOWNLOAD_FILEPATH = Path.cwd().joinpath('../data/raw/')\n",
    "DOWNLOAD_FILEPATH.mkdir(parents=True, exist_ok=True)\n",
    "METADATA_DIR = Path.cwd().joinpath('../scraper_metadata')\n",
    "METADATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EPISODE_PMAP_FILEPATH = str(METADATA_DIR.joinpath('episode_priority_map.txt'))\n",
    "TEAM_PMAP_FILEPATH = str(METADATA_DIR.joinpath('team_priority_map.txt'))\n",
    "EPISODE_DOWNLOAD_FILEPATH = str(METADATA_DIR.joinpath('episode_downloads.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# class ApiRequestLimit(object):\n",
    "    \n",
    "#     def __init__(self, api, request_limit):\n",
    "#         class ProxyObj():\n",
    "#             def __init__(self, parent):\n",
    "#                 self._parent = parent\n",
    "    \n",
    "#             def _consume_request(self, fn, *args, **kwargs):\n",
    "#                 if 0 >= self._parent.request_budget:\n",
    "#                     raise Exception('Request budget exceeded')\n",
    "#                 self._parent.request_budget = self._parent.request_budget - 1\n",
    "#                 return fn(*args, **kwargs)\n",
    "    \n",
    "#         self._api = api\n",
    "#         self.request_limit = request_limit\n",
    "#         self.request_budget = request_limit\n",
    "#         for name, value in inspect.getmembers(api):\n",
    "#             is_subclass_instance = any([isinstance(value, subclass) for _, subclass in inspect.getmembers(api, inspect.isclass)])\n",
    "#             if not hasattr(self, name) and is_subclass_instance:\n",
    "#                 print('{}, {} is sub inst'.format(name, value))   \n",
    "#                 obj = ProxyObj(self)\n",
    "#                 for fn_name, fn in inspect.getmembers(value, callable):\n",
    "#                     if not fn_name.startswith('_'):\n",
    "#                         print('{}.{}'.format(fn_name, fn))\n",
    "#                         setattr(obj, fn_name, lambda *args, **kwargs: obj._consume_request(fn, *args, **kwargs))\n",
    "#                 setattr(self, name, obj)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# w = ApiRequestLimit(api, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# w.request_budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# w.episodes.team(ARBITRARY_TEAM_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:22: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "WARNING:root:Failed to synchronize from disk. File /Users/iainwong/Documents/Development/kaggle/halite/notebooks/../scraper_metadata/team_priority_map.txt is empty.\n",
      "WARNING:root:Failed to synchronize from disk. File /Users/iainwong/Documents/Development/kaggle/halite/notebooks/../scraper_metadata/episode_priority_map.txt is empty.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  if __name__ == '__main__':\n",
      "WARNING:root:Failed to synchronize from disk. File /Users/iainwong/Documents/Development/kaggle/halite/notebooks/../scraper_metadata/episode_downloads.txt is empty.\n",
      "INFO:root:Discovering episodes by arbitrary team_id 5118174\n",
      "INFO:root:Scraping started.\n",
      "INFO:root:Requesting metadata for Team ID 5118174 with priority -1\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.kaggle.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.kaggle.com:443 \"POST /requests/EpisodeService/ListEpisodes?datatype=json HTTP/1.1\" 200 None\n",
      "INFO:root:Downloaded metadata by Team ID 5118174\n",
      "INFO:root:Requesting metadata for Team ID 5118779 with priority 1\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.kaggle.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.kaggle.com:443 \"POST /requests/EpisodeService/ListEpisodes?datatype=json HTTP/1.1\" 200 None\n",
      "INFO:root:Downloaded metadata by Team ID 5118779\n",
      "INFO:root:Requesting metadata for Team ID 5133228 with priority 2\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.kaggle.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.kaggle.com:443 \"POST /requests/EpisodeService/ListEpisodes?datatype=json HTTP/1.1\" 200 None\n",
      "INFO:root:Downloaded metadata by Team ID 5133228\n",
      "INFO:root:Requesting metadata for Team ID 4820508 with priority 3\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.kaggle.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.kaggle.com:443 \"POST /requests/EpisodeService/ListEpisodes?datatype=json HTTP/1.1\" 200 None\n",
      "INFO:root:Downloaded metadata by Team ID 4820508\n",
      "INFO:root:Requesting metadata for Team ID 5124482 with priority 4\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.kaggle.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.kaggle.com:443 \"POST /requests/EpisodeService/ListEpisodes?datatype=json HTTP/1.1\" 200 None\n",
      "INFO:root:Downloaded metadata by Team ID 5124482\n",
      "INFO:root:Requesting replay for Episode ID 1629287 with priority 1216.9133466256485\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.kaggle.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.kaggle.com:443 \"POST /requests/EpisodeService/GetEpisodeReplay?datatype=json HTTP/1.1\" 200 None\n",
      "INFO:root:Downloaded Episode ID 1629287\n",
      "INFO:root:Requesting replay for Episode ID 1590963 with priority 1215.3543081007883\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.kaggle.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.kaggle.com:443 \"POST /requests/EpisodeService/GetEpisodeReplay?datatype=json HTTP/1.1\" 200 None\n",
      "INFO:root:Downloaded Episode ID 1590963\n",
      "INFO:root:Requesting replay for Episode ID 1532745 with priority 1215.1422555819402\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.kaggle.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.kaggle.com:443 \"POST /requests/EpisodeService/GetEpisodeReplay?datatype=json HTTP/1.1\" 200 None\n",
      "INFO:root:Downloaded Episode ID 1532745\n",
      "INFO:root:Requesting replay for Episode ID 1607615 with priority 1214.4796548753156\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.kaggle.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.kaggle.com:443 \"POST /requests/EpisodeService/GetEpisodeReplay?datatype=json HTTP/1.1\" 200 None\n",
      "INFO:root:Downloaded Episode ID 1607615\n",
      "INFO:root:Requesting replay for Episode ID 1630538 with priority 1212.707095858318\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.kaggle.com:443\n",
      "DEBUG:urllib3.connectionpool:https://www.kaggle.com:443 \"POST /requests/EpisodeService/GetEpisodeReplay?datatype=json HTTP/1.1\" 200 None\n",
      "INFO:root:Downloaded Episode ID 1630538\n",
      "INFO:root:Scraping finished\n"
     ]
    }
   ],
   "source": [
    "request_budget = REQUEST_LIMIT\n",
    "teams_priorities = synchronize_disk(TEAM_PMAP_FILEPATH, dict())\n",
    "episodes_priorities = synchronize_disk(EPISODE_PMAP_FILEPATH, dict())\n",
    "scraped_episodes = synchronize_disk(EPISODE_DOWNLOAD_FILEPATH, set())\n",
    "scraped_teams = set()\n",
    "\n",
    "teams_queue = get_min_weighted_priority_queue(teams_priorities, TEAM_WATERMARK)\n",
    "episodes_queue = get_max_weighted_priority_queue(teams_priorities, EPISODE_WATERMARK, request_budget)\n",
    "\n",
    "api = KaggleClient()\n",
    "request_counter = 0\n",
    "request_start = time.time()\n",
    "\n",
    "if 0 < len(teams_queue):\n",
    "    logging.warn('Discovering episodes by prioritized team queue')\n",
    "else:\n",
    "    logging.info('Discovering episodes by arbitrary team_id {}'.format(ARBITRARY_TEAM_ID))\n",
    "    teams_queue = [(ARBITRARY_TEAM_ID, -1)]\n",
    "\n",
    "logging.info('Scraping started.')\n",
    "is_scraping = True\n",
    "while is_scraping:\n",
    "    # scrape episodes\n",
    "    while 0 < len(episodes_queue) and 0 < request_budget:\n",
    "        episode_id, episode_priority = episodes_queue.pop(0)\n",
    "        logging.info('Requesting replay for Episode ID {} with priority {}'.format(episode_id, episode_priority))\n",
    "\n",
    "        # TODO: os.environ['DOWNLOAD_FILEPATH']\n",
    "        filepath = str(DOWNLOAD_FILEPATH.joinpath('replay_EPISODEID_{}_{}.json'.format(episode_id, time.time())))\n",
    "        response = api.replay.episode(episode_id)\n",
    "        save_json(filepath, response)\n",
    "\n",
    "        logging.info('Downloaded Episode ID {}'.format(episode_id))\n",
    "        scraped_episodes.add(episode_id)\n",
    "        episodes_priorities.pop(episode_id, None)\n",
    "        \n",
    "        request_budget = request_budget - 1\n",
    "        request_counter = request_counter + 1\n",
    "        if 60 <= request_counter:\n",
    "            idle_time = 60 - (time.time() - request_start)\n",
    "            if 0 < idle_time:\n",
    "                logging.info('Idling for {} seconds'.format(idle_time))\n",
    "                time.sleep(idle_time)\n",
    "            request_start = time.time()\n",
    "            request_counter = 0 \n",
    "\n",
    "    # find episodes\n",
    "    discovery_limit = (request_budget * REQUEST_DISCOVERY_BUDGET)//1\n",
    "    discovery_budget = discovery_limit\n",
    "    while 0 < discovery_budget and 0 < len(teams_queue):\n",
    "        team_id, team_priority = teams_queue.pop(0)\n",
    "        logging.info('Requesting metadata for Team ID {} with priority {}'.format(team_id, team_priority))\n",
    "\n",
    "        # TODO: os.environ['DOWNLOAD_FILEPATH']\n",
    "        filepath = str(DOWNLOAD_FILEPATH.joinpath('metadata_TEAMID_{}_{}.json'.format(team_id, time.time())))\n",
    "        response = api.episodes.team(team_id)\n",
    "        save_json(filepath, response)\n",
    "        logging.info('Downloaded metadata by Team ID {}'.format(team_id))\n",
    "        scraped_teams.add(team_id)\n",
    "\n",
    "        teams_priorities.update(get_team_priority_map(response))\n",
    "        # TODO: os.environ\n",
    "        teams_priorities = synchronize_priority_map(teams_priorities, TEAM_PMAP_FILEPATH, scraped_teams)\n",
    "        #logging.info('Updated teams pmap to {}'.format(teams_priorities))\n",
    "        # TODO: os.environ\n",
    "        teams_queue = get_min_weighted_priority_queue(teams_priorities, TEAM_WATERMARK)\n",
    "        #logging.info('Updated teams queue to {}'.format(teams_queue))\n",
    "\n",
    "        episodes_priorities.update(get_episode_priority_map(response))\n",
    "        # TODO: os.environ\n",
    "        episodes_priorities = synchronize_priority_map(episodes_priorities, EPISODE_PMAP_FILEPATH, scraped_episodes)\n",
    "        # TODO: os.environ\n",
    "        episodes_queue = get_max_weighted_priority_queue(episodes_priorities, EPISODE_WATERMARK, request_budget)\n",
    "        discovery_budget = discovery_budget - 1\n",
    "        \n",
    "        request_counter = request_counter + 1\n",
    "        if 60 <= request_counter:\n",
    "            idle_time = 60 - (time.time() - request_start)\n",
    "            if 0 < idle_time:\n",
    "                logging.info('Idling for {} seconds'.format(idle_time))\n",
    "                time.sleep(idle_time)\n",
    "            request_start = time.time()\n",
    "            request_counter = 0 \n",
    "    \n",
    "    request_budget = request_budget - discovery_limit + discovery_budget\n",
    "    \n",
    "    # update scrape state\n",
    "    is_scraping = 0 < len(episodes_queue) and 0 < request_budget \n",
    "\n",
    "synchronize_disk(TEAM_PMAP_FILEPATH, teams_priorities, overwrite=True)\n",
    "synchronize_disk(EPISODE_PMAP_FILEPATH, episodes_priorities, overwrite=True)\n",
    "synchronize_disk(EPISODE_DOWNLOAD_FILEPATH, scraped_episodes, overwrite=True)\n",
    "logging.info('Scraping finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# TODO: fix team rank"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "halite",
   "language": "python",
   "name": "halite"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
